import sys
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns
import cv2
from pathlib import Path

sys.path.append('src')
from traditional_ml import extract_enhanced_features, extract_hog_features, train_svm_classifier, evaluate_model, save_model

def load_data_consistent():
    """
    Load data ensuring consistent classes between train and test
    """
    print("ðŸ“ Loading data with consistent classes...")
    
    def load_images(folder_path, img_size=(128, 128), allowed_classes=None):
        images = []
        labels = []
        class_names = []
        
        folder = Path(folder_path)
        class_dirs = [d for d in folder.iterdir() if d.is_dir()]
        
        for class_dir in sorted(class_dirs):
            class_name = class_dir.name
            
            if allowed_classes and class_name not in allowed_classes:
                continue
                
            if class_name not in class_names:
                class_names.append(class_name)
            
            class_idx = class_names.index(class_name)
            
            print(f"  Loading {class_name}...")
            
            for img_path in class_dir.glob('*.[jJ][pP][gG]'):
                img = cv2.imread(str(img_path))
                if img is not None:
                    img = cv2.resize(img, img_size)
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    images.append(img)
                    labels.append(class_idx)
        
        return np.array(images), np.array(labels), class_names
    
    # First load train data to get the classes
    X_train, y_train, train_classes = load_images('TRAIN')
    print(f"âœ… Training: {len(X_train)} images, {len(train_classes)} classes")
    
    # Then load test data, but only include classes that exist in training
    X_test, y_test, test_classes = load_images('TEST', allowed_classes=train_classes)
    print(f"âœ… Test: {len(X_test)} images (filtered to match training classes)")
    
    print(f"ðŸ“ Classes: {train_classes}")
    return X_train, y_train, X_test, y_test, train_classes

def analyze_hyperparameter_sensitivity(X_train, y_train, X_test, y_test, param_name='C', param_range=[0.1, 1, 10, 100]):
    """
    è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æž
    """
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler
    import matplotlib.pyplot as plt
    
    train_scores = []
    test_scores = []
    
    for param_value in param_range:
        print(f"Testing {param_name} = {param_value}")
        
        # è®­ç»ƒæ¨¡åž‹
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        if param_name == 'C':
            svm = SVC(C=param_value, kernel='rbf', gamma='scale', random_state=42)
        else:
            svm = SVC(C=1.0, kernel=param_value, gamma='scale', random_state=42)
        
        svm.fit(X_train_scaled, y_train)
        
        # è®¡ç®—å‡†ç¡®çŽ‡
        train_accuracy = svm.score(X_train_scaled, y_train)
        test_accuracy = svm.score(X_test_scaled, y_test)
        
        train_scores.append(train_accuracy)
        test_scores.append(test_accuracy)
    
    # ç»˜åˆ¶å›¾è¡¨
    plt.figure(figsize=(10, 6))
    plt.plot(param_range, train_scores, 'o-', label='Training Accuracy', linewidth=2)
    plt.plot(param_range, test_scores, 's-', label='Test Accuracy', linewidth=2)
    plt.xlabel(param_name)
    plt.ylabel('Accuracy')
    plt.title(f'Hyperparameter Sensitivity: {param_name}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ä¿å­˜å›¾è¡¨
    os.makedirs('reports', exist_ok=True)
    plt.savefig(f'reports/hyperparameter_{param_name}.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # è¿”å›žæœ€ä½³å‚æ•°
    best_idx = np.argmax(test_scores)
    best_param = param_range[best_idx]
    best_score = test_scores[best_idx]
    
    print(f"ðŸŽ¯ Best {param_name}: {best_param}, Accuracy: {best_score:.4f}")
    return best_param, best_score, train_scores, test_scores

def feature_ablation_analysis(X_train, y_train, X_test, y_test, class_names):
    """
    ç‰¹å¾æ¶ˆèžå®žéªŒåˆ†æž
    """
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score
    import matplotlib.pyplot as plt
    
    # ä¸åŒçš„ç‰¹å¾ç»„åˆ (éœ€è¦æ ¹æ®ä½ çš„ç‰¹å¾ç»´åº¦è°ƒæ•´)
    # è¿™é‡Œä½¿ç”¨ç®€å•çš„ç‰¹å¾åˆ†å‰²ï¼Œä½ å¯ä»¥æ ¹æ®å®žé™…ç‰¹å¾ç»´åº¦è°ƒæ•´
    total_features = X_train.shape[1]
    feature_sets = {
        'HOG Only': (X_train[:, :1800], X_test[:, :1800]),  # å‡è®¾å‰1800ä¸ªæ˜¯HOGç‰¹å¾
        'Color Only': (X_train[:, 1800:2400], X_test[:, 1800:2400]),  # å‡è®¾æŽ¥ä¸‹æ¥æ˜¯é¢œè‰²ç‰¹å¾
        'All Features': (X_train, X_test)
    }
    
    accuracies = []
    names = []
    
    for name, (X_train_sub, X_test_sub) in feature_sets.items():
        print(f"Testing {name}...")
        
        # æ£€æŸ¥ç‰¹å¾ç»´åº¦
        if X_train_sub.shape[1] == 0:
            print(f"  Skipping {name} - no features")
            continue
            
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_sub)
        X_test_scaled = scaler.transform(X_test_sub)
        
        svm = SVC(C=10.0, kernel='rbf', random_state=42)
        svm.fit(X_train_scaled, y_train)
        
        y_pred = svm.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        
        accuracies.append(accuracy)
        names.append(name)
        print(f"  {name}: {accuracy:.4f}")
    
    # ç»˜åˆ¶æŸ±çŠ¶å›¾
    plt.figure(figsize=(10, 6))
    colors = ['lightblue', 'lightgreen', 'lightcoral']
    bars = plt.bar(names, accuracies, color=colors, alpha=0.8)
    
    plt.title('Feature Ablation Study', fontsize=14, fontweight='bold')
    plt.ylabel('Accuracy')
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0, 1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, acc in zip(bars, accuracies):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('reports/feature_ablation.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return dict(zip(names, accuracies))

def detailed_failure_analysis(classifier, scaler, X_test, y_test, class_names):
    """
    è¯¦ç»†çš„å¤±è´¥æ¡ˆä¾‹åˆ†æž
    """
    from sklearn.metrics import confusion_matrix
    import seaborn as sns
    import matplotlib.pyplot as plt
    
    X_test_scaled = scaler.transform(X_test)
    y_pred = classifier.predict(X_test_scaled)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
               xticklabels=class_names, yticklabels=class_names)
    plt.title('Detailed Confusion Matrix - Failure Analysis', fontsize=14, fontweight='bold')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig('reports/detailed_confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # æ‰¾å‡ºæœ€å¸¸æ··æ·†çš„ç±»åˆ«å¯¹
    confusion_pairs = []
    for i in range(len(class_names)):
        for j in range(len(class_names)):
            if i != j and cm[i, j] > 0:
                confusion_pairs.append((i, j, cm[i, j]))
    
    confusion_pairs.sort(key=lambda x: x[2], reverse=True)
    
    print("\nðŸ” Top Confusing Class Pairs:")
    for i, j, count in confusion_pairs[:5]:
        print(f"  {class_names[i]} â†’ {class_names[j]}: {count} cases")
    
    return confusion_pairs

def generate_comprehensive_analysis(y_true, y_pred, class_names, accuracy, X_train, y_train):
    """
    Generate comprehensive analysis and visualizations
    """
    print("Creating comprehensive visualizations...")
    os.makedirs('reports', exist_ok=True)
    
    # Create a comprehensive figure
    fig = plt.figure(figsize=(20, 16))
    
    # 1. Confusion Matrix
    ax1 = plt.subplot2grid((3, 2), (0, 0), colspan=2)
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
                xticklabels=class_names, yticklabels=class_names)
    ax1.set_title(f'Confusion Matrix - Enhanced Traditional ML\nOverall Accuracy: {accuracy:.4f}', 
                  fontsize=14, fontweight='bold')
    ax1.set_xlabel('Predicted Label', fontsize=12)
    ax1.set_ylabel('True Label', fontsize=12)
    ax1.tick_params(axis='x', rotation=45)
    ax1.tick_params(axis='y', rotation=0)
    
    # 2. Per-class Accuracy
    ax2 = plt.subplot2grid((3, 2), (1, 0))
    class_accuracy = []
    for i in range(len(class_names)):
        mask = (y_true == i)
        if np.sum(mask) > 0:
            class_acc = np.sum((y_true == i) & (y_pred == i)) / np.sum(mask)
            class_accuracy.append(class_acc)
        else:
            class_accuracy.append(0)
    
    # Color coding based on performance
    colors = []
    for acc in class_accuracy:
        if acc > 0.7:
            colors.append('#2E8B57')  # Green - good
        elif acc > 0.4:
            colors.append('#FFA500')  # Orange - medium
        else:
            colors.append('#DC143C')  # Red - poor
    
    bars = ax2.bar(range(len(class_names)), class_accuracy, color=colors, alpha=0.8)
    ax2.axhline(y=accuracy, color='red', linestyle='--', linewidth=2, 
                label=f'Overall Accuracy: {accuracy:.3f}')
    ax2.set_title('Accuracy per Class', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Classes')
    ax2.set_ylabel('Accuracy')
    ax2.set_ylim(0, 1)
    ax2.legend()
    ax2.tick_params(axis='x', rotation=45)
    
    # Add values on bars
    for bar, acc in zip(bars, class_accuracy):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{acc:.2f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    # 3. Class Distribution
    ax3 = plt.subplot2grid((3, 2), (1, 1))
    train_counts = [np.sum(y_train == i) for i in range(len(class_names))]
    test_counts = [np.sum(y_true == i) for i in range(len(class_names))]
    
    x = np.arange(len(class_names))
    width = 0.35
    
    bars1 = ax3.bar(x - width/2, train_counts, width, label='Train', alpha=0.7, color='blue')
    bars2 = ax3.bar(x + width/2, test_counts, width, label='Test', alpha=0.7, color='orange')
    
    ax3.set_title('Class Distribution (Train vs Test)', fontsize=12, fontweight='bold')
    ax3.set_xlabel('Classes')
    ax3.set_ylabel('Number of Samples')
    ax3.set_xticks(x)
    ax3.set_xticklabels(class_names, rotation=45, ha='right')
    ax3.legend()
    
    # 4. Error Analysis
    ax4 = plt.subplot2grid((3, 2), (2, 0), colspan=2)
    error_rates = []
    for i in range(len(class_names)):
        total = np.sum(y_true == i)
        if total > 0:
            correct = np.sum((y_true == i) & (y_pred == i))
            error_rate = (total - correct) / total
            error_rates.append(error_rate)
        else:
            error_rates.append(0)
    
    # Color coding for error rates
    error_colors = []
    for rate in error_rates:
        if rate < 0.3:
            error_colors.append('#2E8B57')  # Green - low error
        elif rate < 0.6:
            error_colors.append('#FFA500')  # Orange - medium error
        else:
            error_colors.append('#DC143C')  # Red - high error
    
    bars = ax4.bar(range(len(class_names)), error_rates, color=error_colors, alpha=0.8)
    ax4.set_title('Error Rate per Class', fontsize=12, fontweight='bold')
    ax4.set_xlabel('Classes')
    ax4.set_ylabel('Error Rate')
    ax4.set_ylim(0, 1)
    ax4.set_xticks(range(len(class_names)))
    ax4.set_xticklabels(class_names, rotation=45, ha='right')
    
    # Add values on bars
    for bar, rate in zip(bars, error_rates):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{rate:.2f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('reports/enhanced_traditional_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

def save_detailed_report(y_true, y_pred, class_names, accuracy, class_accuracy, error_rates):
    """
    Save detailed text report
    """
    report_path = 'reports/enhanced_traditional_report.txt'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("PLANTDOC DISEASE CLASSIFICATION - ENHANCED TRADITIONAL ML REPORT\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("EXPERIMENT SUMMARY\n")
        f.write("-" * 20 + "\n")
        f.write("Method: Enhanced Feature Extraction (HOG + Color Histogram + LBP) + SVM\n")
        f.write(f"Total Test Samples: {len(y_true)}\n")
        f.write(f"Number of Classes: {len(class_names)}\n")
        f.write(f"Overall Accuracy: {accuracy:.4f}\n")
        f.write(f"Correct Predictions: {np.sum(y_true == y_pred)}\n")
        f.write(f"Wrong Predictions: {np.sum(y_true != y_pred)}\n\n")
        
        f.write("PER-CLASS PERFORMANCE ANALYSIS\n")
        f.write("-" * 30 + "\n")
        f.write(f"{'Class':<30} {'Accuracy':<10} {'Error Rate':<12} {'Samples':<10} {'Status':<15}\n")
        f.write("-" * 80 + "\n")
        
        for i, class_name in enumerate(class_names):
            samples = np.sum(y_true == i)
            status = "Good" if class_accuracy[i] > 0.7 else "Medium" if class_accuracy[i] > 0.4 else "Needs Improvement"
            f.write(f"{class_name:<30} {class_accuracy[i]:<10.4f} {error_rates[i]:<12.4f} {samples:<10} {status:<15}\n")

def generate_experiment_report(accuracy, best_C, best_score, ablation_results, confusion_pairs, class_names):
    """
    ç”Ÿæˆå®žéªŒåˆ†æžæŠ¥å‘Š
    """
    os.makedirs('reports', exist_ok=True)
    
    with open('reports/experiment_analysis_report.txt', 'w') as f:
        f.write("TRADITIONAL ML EXPERIMENT ANALYSIS REPORT\n")
        f.write("=" * 50 + "\n\n")
        
        f.write("OVERALL RESULTS\n")
        f.write("-" * 20 + "\n")
        f.write(f"Final Test Accuracy: {accuracy:.4f}\n\n")
        
        f.write("HYPERPARAMETER ANALYSIS\n")
        f.write("-" * 25 + "\n")
        f.write(f"Best C parameter: {best_C}\n")
        f.write(f"Best validation accuracy: {best_score:.4f}\n\n")
        
        f.write("FEATURE ABLATION RESULTS\n")
        f.write("-" * 28 + "\n")
        for feature_set, acc in ablation_results.items():
            f.write(f"{feature_set}: {acc:.4f}\n")
        f.write("\n")
        
        f.write("FAILURE ANALYSIS\n")
        f.write("-" * 20 + "\n")
        f.write("Top confusing class pairs:\n")
        for i, j, count in confusion_pairs[:5]:
            f.write(f"  {class_names[i]} â†’ {class_names[j]}: {count} cases\n")
    
    print("âœ… Experiment report saved to 'reports/experiment_analysis_report.txt'")

def main():
    """
    Main traditional ML pipeline with experiment analysis
    """
    print("ðŸš€ TRADITIONAL ML WITH EXPERIMENT ANALYSIS")
    print("=" * 60)
    
    try:
        # 1. Load data with consistent classes
        X_train, y_train, X_test, y_test, class_names = load_data_consistent()
        
        # 2. Extract enhanced features
        print("\nðŸ” Extracting Enhanced Features...")
        X_train_features = extract_enhanced_features(X_train)
        X_test_features = extract_enhanced_features(X_test)
        
        print(f"âœ… Enhanced features - Train: {X_train_features.shape}, Test: {X_test_features.shape}")
        
        # 3. Train SVM with improved parameters
        print("\nðŸ¤– Training SVM Classifier...")
        svm_classifier, scaler = train_svm_classifier(
            X_train_features, y_train, 
            C=10.0, kernel='rbf', gamma='scale', use_grid_search=False
        )
        print("âœ… SVM training completed!")
        
        # 4. Evaluate model
        print("\nðŸ“Š Evaluating Model...")
        accuracy, report, y_pred = evaluate_model(
            svm_classifier, scaler, X_test_features, y_test, class_names
        )
        
        print(f"ðŸŽ¯ TEST ACCURACY: {accuracy:.4f}")
        print("\nClassification Report:")
        print(report)
        
        # 5. Create comprehensive visualization
        print("\nðŸ“ˆ Creating Comprehensive Analysis...")
        generate_comprehensive_analysis(y_test, y_pred, class_names, accuracy, X_train, y_train)
        
        # =============================================
        # ðŸ†• æ–°å¢žï¼šå®žéªŒåˆ†æžéƒ¨åˆ†
        # =============================================
        
        print("\n" + "="*50)
        print("ðŸ§ª EXPERIMENTAL ANALYSIS")
        print("="*50)
        
        # 1. è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æž
        print("\n1. Hyperparameter Sensitivity Analysis...")
        best_C, best_score, train_scores, test_scores = analyze_hyperparameter_sensitivity(
            X_train_features, y_train, X_test_features, y_test,
            param_name='C', param_range=[0.1, 1, 10, 100, 1000]
        )
        
        # 2. ç‰¹å¾æ¶ˆèžå®žéªŒ
        print("\n2. Feature Ablation Analysis...")
        ablation_results = feature_ablation_analysis(
            X_train_features, y_train, X_test_features, y_test, class_names
        )
        
        # 3. å¤±è´¥æ¡ˆä¾‹åˆ†æž
        print("\n3. Failure Case Analysis...")
        confusion_pairs = detailed_failure_analysis(
            svm_classifier, scaler, X_test_features, y_test, class_names
        )
        
        # 4. ä¿å­˜æ¨¡åž‹
        print("\nðŸ’¾ Saving model...")
        save_model(svm_classifier, scaler, model_path='models/traditional_ml_enhanced')
        
        # 5. ç”Ÿæˆå®žéªŒæŠ¥å‘Š
        print("\nðŸ“„ Generating experiment report...")
        generate_experiment_report(accuracy, best_C, best_score, ablation_results, confusion_pairs, class_names)
        
        print(f"\nðŸŽ‰ TRADITIONAL ML WITH EXPERIMENT ANALYSIS COMPLETED!")
        print(f"ðŸ“Š Final Accuracy: {accuracy:.4f}")
        print(f"ðŸ”§ Best C parameter: {best_C}")
        print(f"ðŸ“ˆ Feature ablation results saved")
        print(f"ðŸ“‰ Failure analysis completed")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()